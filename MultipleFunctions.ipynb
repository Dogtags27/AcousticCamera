{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import acoular\n",
    "from pylab import figure, plot, axis, imshow, colorbar, show, savefig, clf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.signal import butter, sosfilt, welch, lfilter\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "#     nyquist = 0.5 * fs\n",
    "#     low = lowcut / nyquist\n",
    "#     high = highcut / nyquist\n",
    "#     b, a = butter(order, [low, high], btype='band')\n",
    "#     return b, a\n",
    "\n",
    "# def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "#     b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     y = lfilter(b, a, data)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration = 5  # Duration of the audio in seconds\n",
    "# sample_rate = 44100  # Sample rate (samples per second)\n",
    "# lower_freq = 2950  # Lower limit frequency in Hz\n",
    "# upper_freq = 3050  # Upper limit frequency in Hz\n",
    "\n",
    "# # Generate time array\n",
    "# t = np.linspace(0, duration, duration * sample_rate, endpoint=False)\n",
    "\n",
    "# # Generate white noise signal\n",
    "# white_noise = np.random.randn(len(t))\n",
    "\n",
    "# # # Design a bandpass filter\n",
    "# # order = 4  # Filter order\n",
    "# # nyquist = 0.5 * sample_rate  # Nyquist frequency\n",
    "# low = lower_freq \n",
    "# high = upper_freq \n",
    "# # sos = butter(order, [low, high], btype='band',output=\"sos\")\n",
    "\n",
    "# # # Apply the bandpass filter to the white noise\n",
    "# # filtered_noise = sosfilt(sos, white_noise)\n",
    "\n",
    "# # # Normalize the audio signal\n",
    "# # filtered_noise /= np.max(np.abs(filtered_noise))\n",
    "# filtered_data = butter_bandpass_filter(white_noise, low, high, sample_rate, order=6)\n",
    "# # Save the audio signal to a WAV file\n",
    "# # wav.write(\"white_noise.wav\", sample_rate, filtered_noise.astype(np.float32))\n",
    "# output_dir = 'c:/Users/Hriday Desai/OneDrive/Desktop/Acoustic/recorder/recorder_output'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_dir = 'c:/Users/Hriday Desai/OneDrive/Desktop/Acoustic/recorder/recorder_output/records'\n",
    "# os.makedirs(output_dir,exist_ok=True)\n",
    "# output_file=\"c:/Users/Hriday Desai/OneDrive/Desktop/Acoustic/recorder/recorder_output/records/audio.wav\"\n",
    "# wav.write(output_file, sample_rate, filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the audio file\n",
    "# sample_rate, audio_data = wav.read('c:/Users/Hriday Desai/OneDrive/Desktop/Acoustic/recorder/recorder_output/records/audio.wav')\n",
    "\n",
    "# # Compute the power spectral density (PSD) using Welch's method\n",
    "# frequencies, psd = welch(audio_data, fs=sample_rate, nperseg=1024)\n",
    "\n",
    "# # Plot the power spectrum\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.semilogy(frequencies, psd)\n",
    "# plt.title('Power Spectral Density')\n",
    "# plt.xlabel('Frequency (Hz)')\n",
    "# plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wav_file='C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/audio.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/frames'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of frames to be processed per second\n",
    "framesProcess = 44100//5  \n",
    "\n",
    "# Open the WAV file and read the audio data\n",
    "sample_rate, audio_data = wav.read(input_wav_file)\n",
    "sample_freq = sample_rate\n",
    "\n",
    "# Calculate the total number of frames based on the frame duration\n",
    "# num_frames = int(len(audio_data) // (sample_rate * frame_duration))\n",
    "num_frames = int(len(audio_data)//framesProcess)\n",
    "\n",
    "# os.makedirs(\"c:/Users/Hriday Desai/OneDrive/Desktop/Acoustic/recorder/recorder_output/split_audio\", exist_ok=True)\n",
    "audio_cuts=\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/audio_cuts\"\n",
    "os.makedirs(audio_cuts,exist_ok=True)\n",
    "\n",
    "\n",
    "# Iterate through each frame and save it as a separate HDF5 file\n",
    "for i in range(num_frames):\n",
    "    # Extract the audio data for the current frame\n",
    "    frame_start = int(i * framesProcess)\n",
    "    frame_end = int((i + 1) * framesProcess)\n",
    "    frame_audio_data = audio_data[frame_start:frame_end]\n",
    "\n",
    "    output_audio_cut_file=f\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/audio_cuts/audio_cut_{i+1}.wav\"\n",
    "    wav.write(output_audio_cut_file, sample_rate, frame_audio_data)\n",
    "    \n",
    "    # Create the output HDF5 file path for the current frame\n",
    "    output_h5_file = f\"{output_dir}/frame_{i + 1}.h5\"\n",
    "\n",
    "    # Create an HDF5 file for the current frame\n",
    "    with h5py.File(output_h5_file, 'w') as hf:\n",
    "        # Create a dataset to store audio data\n",
    "        hf.create_dataset('/time_data', data=frame_audio_data, dtype=np.int16)\n",
    "\n",
    "        # Add attribute for sample rate\n",
    "        hf['/time_data'].attrs['sample_freq'] = sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of microphones: 16\n"
     ]
    }
   ],
   "source": [
    "micgeofile = 'C:/Users/NITR/Desktop/Acoustic/recorder/xml/microphone.xml'\n",
    "\n",
    "mg = acoular.MicGeom( from_file=micgeofile )\n",
    "\n",
    "# Check the number of microphones\n",
    "num_mics = mg.num_mics\n",
    "print(\"Number of microphones:\", num_mics)\n",
    "\n",
    "# Define the rectilinear grid\n",
    "# horizontal angle of the camera is 28 deg and vertical angle is 19deg.\n",
    "# Relation between z and x(min,max): x = (+- z//4)\n",
    "# Relation between z and y(min,max): y = (+- z//6)\n",
    "z=3\n",
    "rg = acoular.RectGrid(x_min=(-1*z*10)//4, x_max=z*10//4, y_min=(-1*z*10)//6, y_max=z*10//6, z=z, increment=0.02)\n",
    "# Define the steering vector\n",
    "st = acoular.SteeringVector(grid=rg, mics=mg)\n",
    "# Compute the beamformer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('frame_1_cache.h5', 1)]\n",
      "[('frame_1_cache.h5', 2)]\n",
      "[('frame_2_cache.h5', 1)]\n",
      "[('frame_2_cache.h5', 2)]\n",
      "[('frame_3_cache.h5', 1)]\n",
      "[('frame_3_cache.h5', 2)]\n",
      "[('frame_4_cache.h5', 1)]\n",
      "[('frame_4_cache.h5', 2)]\n",
      "[('frame_5_cache.h5', 1)]\n",
      "[('frame_5_cache.h5', 2)]\n",
      "[('frame_6_cache.h5', 1)]\n",
      "[('frame_6_cache.h5', 2)]\n",
      "[('frame_7_cache.h5', 1)]\n",
      "[('frame_7_cache.h5', 2)]\n",
      "[('frame_8_cache.h5', 1)]\n",
      "[('frame_8_cache.h5', 2)]\n",
      "[('frame_9_cache.h5', 1)]\n",
      "[('frame_9_cache.h5', 2)]\n",
      "[('frame_10_cache.h5', 1)]\n",
      "[('frame_10_cache.h5', 2)]\n",
      "[('frame_11_cache.h5', 1)]\n",
      "[('frame_11_cache.h5', 2)]\n",
      "[('frame_12_cache.h5', 1)]\n",
      "[('frame_12_cache.h5', 2)]\n",
      "[('frame_13_cache.h5', 1)]\n",
      "[('frame_13_cache.h5', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/frame_images\", exist_ok=True)\n",
    "audio_files_count=0\n",
    "for i in range(num_frames):\n",
    "    output_h5_file = f\"{output_dir}/frame_{i + 1}.h5\"\n",
    "    datafile = output_h5_file\n",
    "    \n",
    "    # audio_cut_file=f\"{audio_cuts}/audio_cut_{i+1}.wav\"\n",
    "    # y, sr = librosa.load(audio_cut_file, sr=None)\n",
    "\n",
    "    # # Compute the FFT (Fast Fourier Transform) to obtain the power spectrum\n",
    "    # fft_result = np.fft.fft(y)\n",
    "\n",
    "    # # Compute the frequencies corresponding to the FFT bins\n",
    "    # fft_freqs = np.fft.fftfreq(len(y), 1/sr)\n",
    "\n",
    "    # # Discard negative frequencies\n",
    "    # positive_fft_freqs = fft_freqs[:len(fft_freqs)//2]\n",
    "    # positive_fft_result = fft_result[:len(fft_result)//2]\n",
    "\n",
    "    # # Find the frequency with the maximum power\n",
    "    # max_power_index = np.argmax(np.abs(positive_fft_result))\n",
    "    # max_power_frequency = positive_fft_freqs[max_power_index]\n",
    "    max_power_frequency = 3000\n",
    "    # print(\"Max Power Frequency:\", max_power_frequency, \"Hz\")\n",
    "    \n",
    "    ts = acoular.TimeSamples(name=datafile)\n",
    "    ps = acoular.PowerSpectra(time_data=ts, block_size=1024, window='Hanning')\n",
    "    bb = acoular.BeamformerMusic(freq_data=ps, steer=st)\n",
    "    # Compute the synthetic sound field\n",
    "    pm = bb.synthetic(max_power_frequency, 3)\n",
    "    # Compute the sound pressure level\n",
    "    Lm = acoular.L_p(pm)\n",
    "    \n",
    "    imshow( Lm.T, origin='lower', vmin=Lm.max()-1, extent=rg.extend() ,interpolation=\"bicubic\", cmap=\"rainbow\")\n",
    "\n",
    "    axis('off')\n",
    "    \n",
    "    output_file = f\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/frame_images/frame_{i+1}.png\"\n",
    "    audio_files_count+=1\n",
    "    # Save the plot as an image file\n",
    "    savefig(output_file, bbox_inches='tight', pad_inches=0)\n",
    "    clf()\n",
    "    # Clear the current plot to prepare for the next frame\n",
    "    # clf()  # Clear the current figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted: 12\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract frames from video\n",
    "video_path=\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/video.avi\"\n",
    "output_folder=\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/imageshots\"\n",
    "frame_rate_ms=400*4\n",
    "\n",
    "def extract_frames(video_path, output_folder, frame_rate_ms):\n",
    "    # Create output folder if not exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    # Get frame rate of the video\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Calculate frame interval based on desired frame rate (in milliseconds)\n",
    "    # frame_interval = int(frame_rate * frame_rate_ms)\n",
    "    \n",
    "    frame_interval = frame_rate_ms\n",
    "    \n",
    "    # Variables to track frame number and current time\n",
    "    frame_count = 0\n",
    "    current_time = 0\n",
    "\n",
    "    # Loop through video frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Check if it's time to save the frame\n",
    "        if current_time >= frame_interval:\n",
    "            # Save frame as image\n",
    "            image_path = os.path.join(output_folder, f\"frame_{1+frame_count}.png\")\n",
    "            cv2.imwrite(image_path, frame)\n",
    "            frame_count += 1\n",
    "            current_time = 0\n",
    "\n",
    "        # Update current time\n",
    "        current_time += int(1000 / frame_rate)\n",
    "\n",
    "        # Move to next frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, cap.get(cv2.CAP_PROP_POS_FRAMES) + 1)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Frames extracted: {frame_count}\")\n",
    "    print(\"Extraction complete.\")\n",
    "    return frame_count\n",
    "frame_count=extract_frames(video_path, output_folder, frame_rate_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_frame_count=min(frame_count,audio_files_count)\n",
    "audio_images_directory=\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/frame_images\"\n",
    "video_images_directory=\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/imageshots\"\n",
    "\n",
    "output_folder=\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(background_img, overlay_img, output_path):\n",
    "    # Read the images\n",
    "    background = cv2.imread(str(background_img))\n",
    "    overlay = cv2.imread(str(overlay_img))\n",
    "\n",
    "    # Resize overlay image to match background dimensions\n",
    "    overlay = cv2.resize(overlay, (background.shape[1], background.shape[0]))\n",
    "    # Horizontally flip the overlay image\n",
    "    # overlay_flipped = cv2.flip(overlay, 1)\n",
    "    overlay_flipped = overlay\n",
    "    # Blend the images using simple addition\n",
    "    blended = cv2.addWeighted(background, 0.3, overlay_flipped, 0.7, 0)\n",
    "\n",
    "    # Write the blended image to the output path\n",
    "    cv2.imwrite(str(output_path), blended)\n",
    "\n",
    "    print(f\"Merged image saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_1.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_2.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_3.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_4.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_5.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_6.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_7.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_8.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_9.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_10.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_11.png\n",
      "Merged image saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_12.png\n",
      "Merge process complete.\n"
     ]
    }
   ],
   "source": [
    "# Specify the directories containing the images\n",
    "imageshots_dir = video_images_directory\n",
    "frame_images_dir = audio_images_directory\n",
    "\n",
    "imageshots_files=[]\n",
    "frame_images_files=[]\n",
    "merged_images_files=[]\n",
    "for i in range(1,merge_frame_count+1):\n",
    "    imageshots_files.append(f\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/imageshots/frame_{i}.png\")\n",
    "    frame_images_files.append(f\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/frame_images/frame_{i}.png\")\n",
    "    merged_images_files.append(f\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images/frame_{i}.png\")\n",
    "\n",
    "output_dir=output_folder\n",
    "# Iterate through the PNG files and merge them\n",
    "# for imageshot_file in imageshots_files:\n",
    "for j in range(1,merge_frame_count+1):\n",
    "    frame_image_path = frame_images_files[j-1]\n",
    "    imageshot_path = imageshots_files[j-1]\n",
    "    merged_image_path = merged_images_files[j-1]\n",
    "    \n",
    "    merge_images(imageshot_path,frame_image_path,merged_image_path)\n",
    "\n",
    "print(\"Merge process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video.mp4.\n",
      "Moviepy - Writing video C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video.mp4\n",
      "Video saved: C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "from pathlib import Path\n",
    "# Specify the directory containing the merged images\n",
    "merged_images_dir = Path(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/merged_images\")\n",
    "\n",
    "# Get a list of image files in the directory\n",
    "image_files = sorted([str(f) for f in merged_images_dir.iterdir() if f.suffix == '.png'])\n",
    "\n",
    "# Specify the frame rate (5 images per second)\n",
    "fps = 5\n",
    "\n",
    "# Create an ImageSequenceClip from the image files\n",
    "clip = ImageSequenceClip(image_files, fps=fps)\n",
    "\n",
    "# Specify the output video file path\n",
    "output_video_path = \"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video.mp4\"\n",
    "\n",
    "# Write the clip to the output video file\n",
    "clip.write_videofile(output_video_path, codec='libx264')\n",
    "\n",
    "print(f\"Video saved: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Power Frequency: 2999.9304347826087 Hz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = \"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/audio.wav\"\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "\n",
    "# Compute the FFT (Fast Fourier Transform) to obtain the power spectrum\n",
    "fft_result = np.fft.fft(y)\n",
    "\n",
    "# Compute the frequencies corresponding to the FFT bins\n",
    "fft_freqs = np.fft.fftfreq(len(y), 1/sr)\n",
    "\n",
    "# Discard negative frequencies\n",
    "positive_fft_freqs = fft_freqs[:len(fft_freqs)//2]\n",
    "positive_fft_result = fft_result[:len(fft_result)//2]\n",
    "\n",
    "# Find the frequency with the maximum power\n",
    "max_power_index = np.argmax(np.abs(positive_fft_result))\n",
    "max_power_frequency = positive_fft_freqs[max_power_index]\n",
    "\n",
    "print(\"Max Power Frequency:\", max_power_frequency, \"Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "# Load the audio file\n",
    "sample_rate, audio_data = wavfile.read(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/audio.wav\")\n",
    "\n",
    "# Transpose the audio data matrix\n",
    "audio_data_transposed = np.transpose(audio_data)\n",
    "\n",
    "# Extract the desired channel (assuming you want the first channel)\n",
    "desired_channel = 0\n",
    "desired_audio_channel = audio_data_transposed[desired_channel]\n",
    "wavfile.write(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/desired_audio_channel.wav\", sample_rate, desired_audio_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video_with_audio.mp4.\n",
      "MoviePy - Writing audio in output_video_with_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - Writing video C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video_with_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video_with_audio.mp4\n",
      "Video with replaced audio saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load the video file\n",
    "video_clip = VideoFileClip(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video.mp4\")\n",
    "\n",
    "# Load the audio file\n",
    "audio_clip = AudioFileClip(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/desired_audio_channel.wav\")\n",
    "\n",
    "# # Extract the desired channel from the audio file (assuming you want the first channel)\n",
    "# desired_channel = 0\n",
    "sample_rate, desired_audio = wav.read(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/desired_audio_channel.wav\")\n",
    "\n",
    "# Synchronize audio and video durations if necessary\n",
    "if video_clip.duration != audio_clip.duration:\n",
    "    # Adjust audio duration to match video duration\n",
    "    desired_audio = desired_audio[:int(video_clip.duration * audio_clip.fps)]\n",
    "\n",
    "# Replace the audio in the video file with the desired audio channel\n",
    "video_clip = video_clip.set_audio(audio_clip)\n",
    "\n",
    "# Write the modified video file with replaced audio\n",
    "video_clip.write_videofile(\"C:/Users/NITR/Desktop/Acoustic/recorder/recorder_output/records/output_video_with_audio.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "print(\"Video with replaced audio saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
